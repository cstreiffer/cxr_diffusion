{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SzOjB2u2l4tn",
        "QzKEP2Zil6gV",
        "xMfNsKw0IxzX",
        "V0lZSEjm__rD"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download the CheXchoNet dataset\n",
        "\n",
        "| Dataset  | Images | Size |\n",
        "| -------- | ------ | ---- |\n",
        "| [CheXchoNet](https://physionet.org/content/chexchonet/1.0.0/)  | 71,589 | 2.7 GB |"
      ],
      "metadata": {
        "id": "4MGp-hlniQNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter physionet username\n",
        "PHYSIONET_USERNAME=input(\"Physionet Username:\")"
      ],
      "metadata": {
        "id": "_QRoqEfNiZes"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the files\n",
        "!wget -r -N -c -np --user {PHYSIONET_USERNAME} --ask-password https://physionet.org/files/chexchonet/1.0.0/"
      ],
      "metadata": {
        "id": "qHUnJStNiUq3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now move the images to the correct path\n",
        "import os\n",
        "OUTPUT_IMAGE_PATH = \"./cxrs/\"\n",
        "os.rename(\"physionet.org/files/chexchonet/1.0.0/images/\", \"./cxrs\")\n",
        "\n",
        "# Now move the csv to a local chexchonet folder\n",
        "import pandas as pd\n",
        "OUTPUT_METADATA_PATH = \"./chexchonet/\"\n",
        "if not os.path.exists(OUTPUT_METADATA_PATH):\n",
        "  os.makedirs(OUTPUT_METADATA_PATH)\n",
        "metadata_df = pd.read_csv(\"physionet.org/files/chexchonet/1.0.0/metadata.csv\")\n",
        "metadata_df.to_csv(os.path.join(OUTPUT_METADATA_PATH, \"metadata.csv\"), index=False)"
      ],
      "metadata": {
        "id": "mIGWS7Ddjt7l"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separate into Training, Validation, and Testing Splits"
      ],
      "metadata": {
        "id": "Lv8Y6HwEjdWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nTcUSUVykYvI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_METADATA_PATH = \"./chexchonet/\"\n",
        "RANDOM_SEED = None\n",
        "\n",
        "# Create the output directory\n",
        "import os\n",
        "if not os.path.exists(OUTPUT_METADATA_PATH):\n",
        "  os.makedirs(OUTPUT_METADATA_PATH)\n",
        "\n",
        "# Seed if defined\n",
        "import random\n",
        "if RANDOM_SEED is not None:\n",
        "  random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "C3PzH7RTkdTC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now load the data\n",
        "chexchonet_df = pd.read_csv(os.path.join(OUTPUT_METADATA_PATH, \"metadata.csv\"))\n",
        "chexchonet_df['file_path'] = chexchonet_df['cxr_filename']\n",
        "\n",
        "def split_list(data, train_split=0.7, test_split=0.2, valid_split=0.1):\n",
        "    if train_split + test_split + valid_split > 1.0:\n",
        "        raise ValueError(\"The splits must sum up to 1.0\")\n",
        "\n",
        "    # Shuffle the list randomly\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Calculate the split indices\n",
        "    train_end = int(train_split * len(data))\n",
        "    test_end = train_end + int(test_split * len(data))\n",
        "\n",
        "    # Split the data\n",
        "    train_data = data[:train_end]\n",
        "    test_data = data[train_end:test_end]\n",
        "    valid_data = data[test_end:]\n",
        "\n",
        "    return set(train_data), set(test_data), set(valid_data)\n",
        "\n",
        "# Spliy into the datasets\n",
        "train, test, valid = split_list(chexchonet_df.patient_id.unique())\n",
        "def map_set(v):\n",
        "  if v in train:\n",
        "    return \"train\"\n",
        "  elif v in test:\n",
        "    return \"test\"\n",
        "  else:\n",
        "    return \"valid\"\n",
        "\n",
        "# Now map and label\n",
        "chexchonet_df['diffusion_set'] = chexchonet_df['patient_id'].apply(map_set)\n",
        "chexchonet_df['inference_set'] = chexchonet_df['patient_id'].apply(map_set)"
      ],
      "metadata": {
        "id": "z2psQ-8VjnGt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now output\n",
        "chexchonet_df[chexchonet_df['diffusion_set'] == \"train\"].to_csv(os.path.join(OUTPUT_METADATA_PATH, 'diffusion_metadata_train.csv'), index=False)\n",
        "chexchonet_df[chexchonet_df['diffusion_set'] == \"test\"].to_csv(os.path.join(OUTPUT_METADATA_PATH, 'diffusion_metadata_test.csv'), index=False)\n",
        "chexchonet_df[chexchonet_df['diffusion_set'] == \"eval\"].to_csv(os.path.join(OUTPUT_METADATA_PATH, 'diffusion_metadata_eval.csv'), index=False)"
      ],
      "metadata": {
        "id": "PXi9u_PVk8bx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the DDPM Model"
      ],
      "metadata": {
        "id": "h6UK_vIUk7RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from run import load_file\n",
        "CONFIG_FILE_PATH = \"src/train/training_configs/class_diffusion_large_224.yaml\"\n",
        "args = load_file(CONFIG_FILE_PATH)"
      ],
      "metadata": {
        "id": "oizn70_inaSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from run import run\n",
        "run(args)"
      ],
      "metadata": {
        "id": "uCdhiswbngSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Synthetic Data"
      ],
      "metadata": {
        "id": "SzOjB2u2l4tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gen_images import gen\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "MODEL_PATH = \"drive/MyDrive/cxr/models/class_diffusion_large_224_8_2024-07-20/\"\n",
        "OUTPUT_DATA_PATH = \"cxr_synthetic\"\n",
        "NUM_BATCHES = 10 # Total number of batches to run\n",
        "BATCH_SIZE = 16 # Modify this based on available GPU RAM\n",
        "\n",
        "# Define custom sample function\n",
        "def sample_context(bs):\n",
        "  #   - age   (norm)\n",
        "  #   - sex_m (one-hot)\n",
        "  #   - sex_f (one-hot)\n",
        "  #   - ivsd  (norm)\n",
        "  #   - lvpwd (norm)\n",
        "  #   - lvidd (norm)\n",
        "  s = [np.random.choice([0,1]) for i in range(bs)]\n",
        "  return [[\n",
        "      np.random.normal(loc=-.5, scale=1.0),\n",
        "      s[i],\n",
        "      1 if s[i] == 0 else 0,\n",
        "      np.random.normal(loc=.5, scale=1.0),\n",
        "      np.random.normal(loc=.5, scale=1.0),\n",
        "      np.random.normal(loc=.5, scale=1.0)\n",
        "  ] for i in range(bs)]\n",
        "\n",
        "df = gen(\n",
        "    MODEL_PATH,\n",
        "    OUTPUT_DATA_PATH,\n",
        "    NUM_BATCHES,\n",
        "    BATCH_SIZE,\n",
        "    sample_fn=sample_context\n",
        ")"
      ],
      "metadata": {
        "id": "iDVw-HUMnjmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "geJ48MSR_YhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Synthetic Data"
      ],
      "metadata": {
        "id": "QzKEP2Zil6gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inception Score"
      ],
      "metadata": {
        "id": "xMfNsKw0IxzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import inception_v3\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def inception_score(imgs, cuda=True, batch_size=32, resize=False, splits=1):\n",
        "    \"\"\"Compute the Inception Score of generated images.\n",
        "\n",
        "    Args:\n",
        "        imgs (List or array): List or array of PIL Images.\n",
        "        cuda (bool): Whether to use GPU.\n",
        "        batch_size (int): Batch size for feeding into Inception v3.\n",
        "        resize (bool): Resize to 299x299 before feeding into Inception.\n",
        "        splits (int): Number of splits.\n",
        "\n",
        "    Returns:\n",
        "        float: The Inception Score.\n",
        "    \"\"\"\n",
        "    assert batch_size > 0\n",
        "    assert len(imgs) > 0\n",
        "    assert splits > 0\n",
        "\n",
        "    # Set up dtype\n",
        "    if cuda:\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        dtype = torch.FloatTensor\n",
        "\n",
        "    # Set up dataloader\n",
        "    if resize:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    imgs = [transform(img) for img in imgs]\n",
        "    imgs = torch.stack(imgs, 0).type(dtype)\n",
        "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
        "\n",
        "    # Load inception model\n",
        "    inception_model = inception_v3(pretrained=True, transform_input=False)\n",
        "    inception_model.eval()\n",
        "    if cuda:\n",
        "        inception_model.cuda()\n",
        "\n",
        "    # Get predictions\n",
        "    preds = []\n",
        "    for batch in dataloader:\n",
        "        with torch.no_grad():\n",
        "            if cuda:\n",
        "                batch = batch.cuda()\n",
        "            pred = inception_model(batch)\n",
        "            if pred.dim() == 1:\n",
        "                pred = pred.unsqueeze(0)\n",
        "            preds.append(pred.cpu().numpy())\n",
        "\n",
        "    # Now compute the mean kl-div\n",
        "    preds = np.concatenate(preds, 0)\n",
        "    scores = []\n",
        "    for i in range(splits):\n",
        "        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
        "        p_yx = np.exp(part - np.max(part, axis=1, keepdims=True))\n",
        "        p_yx /= p_yx.sum(axis=1, keepdims=True)\n",
        "        p_y = np.mean(p_yx, axis=0)\n",
        "        scores.append(entropy(p_yx, p_y, axis=1).mean())\n",
        "\n",
        "    return np.exp(np.mean(scores))"
      ],
      "metadata": {
        "id": "Xu6BuM-_Izbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_is = inception_score(train_images, cuda=True)\n",
        "test_is = inception_score(test_images, cuda=True)\n",
        "gen_is = inception_score(gen_images, cuda=True)\n",
        "\n",
        "print(f\"Train Score: {train_is:0.4f}\")\n",
        "print(f\"Test Score: {test_is:0.4f}\")\n",
        "print(f\"Gen Score: {gen_is:0.4f}\")"
      ],
      "metadata": {
        "id": "BLsnlNVTIzeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FID Score"
      ],
      "metadata": {
        "id": "V0lZSEjm__rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "id": "93g1CIkN_891"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid {OUTPUT_DATA_PATH} images/chexchonet_train/ --device cuda:0"
      ],
      "metadata": {
        "id": "kh6FZ_6eACoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}